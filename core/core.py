#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pympler import tracker
import configparser
from datetime import datetime
from core.gtfobins_scraper import get_gtfobin
import time
from core.redis_client import RedisClient
from database.models import Run, Runs
from core.ssh_session import Session
import logging
import sys
from core.constants import EX, EXTERNAL, IG, INTERNAL, VA, ModuleStages, RunStatus
import importlib
import json
import socket
from threading import Thread
from multiprocessing import Process
from core.helper import (
    create_new_target,
    generate_random,
    get_module,
    get_module_config,
    get_module_types,
    logger_class,
    os_name,
    python_version,
    remove_suffix,
    tool_config,
)
from database.db import check_db, db_con
import argparse
import os
import gc 

from jumpssh.exception import ConnectionError

logger = logging.getLogger(__name__)

# Process specific variables
_session = None  # Holds SSH Session object
_db_run = None  # Run Row input for specific run
_redis = None
_latest_scan = None
timer_start = 0  # Timer for run


"""
Get scan id
"""

def get_scan_id():
    global _db_run

    return _db_run.id

"""
Initializes the program
"""


def load():
    # Get args
    # args = get_args()

    # Read from command line
    while True:
        target = read_cli()

        if target is not False:
            # Create process
            p = Process(
                target=scanner_start,
                args=(
                    target,
                ),
            )
            p.start()


"""
Function to get all modules for specific target with a certain type
Adds thread to threads array
"""


def thread_module_types(threads, config, type_, target_type):
    global _session

    logger.info(f"Running {target_type} {type_}")

    # Append modules
    for module in get_module_types(type_, target_type):
        module = module.lower()
        # Check if is in config file
        if config.has_section(module.capitalize()):
            module_config = config[module.capitalize()]

            # Check if module should run
            if module_config["enabled"] == "True":
                # Check type, use Nmap result to run specific tools
                cls = get_module(module)

                # Run IG independantly from others
                if type_ == IG:
                    threads.append(
                        Thread(
                            target=run_module,
                            args=(
                                cls(
                                    get_module_config(config, module),
                                    config["General"],
                                    _session,
                                    {},
                                ),
                            ),
                        )
                    )
                else:
                    # Check if Nmap result exists, add to payload
                    if cls._target_type == EXTERNAL:
                        # Check which tools that can be executed, send payload with ports
                        try:
                            data = {}
                            with open(
                                f"{os.getcwd()}/targets/{config['General']['target']}/results/nmap_result.json",
                                "r"
                            ) as f:
                                data = json.load(f)

                            payload = {"ports": []}

                            # Make into list
                            if type(data["nmaprun"]["host"]["ports"]["port"]) is dict:
                                data["nmaprun"]["host"]["ports"]["port"] = [data["nmaprun"]["host"]["ports"]["port"]]
                            
                            # add all services
                            if "all" in cls._services:
                                for port in data["nmaprun"]["host"]["ports"]["port"]:
                                    if port["state"]["@state"] == "open":
                                        payload["ports"].append((port["@portid"], port["service"]["@name"]))
                            # add ports with specific services
                            else:
                                for port in data["nmaprun"]["host"]["ports"]["port"]:
                                    if (
                                        port["service"]["@name"] in cls._services
                                        and port["state"]["@state"] == "open"
                                    ):
                                        payload["ports"].append((port["@portid"], port["service"]["@name"]))

                            threads.append(
                                Thread(
                                    target=run_module,
                                    args=(
                                        cls(
                                            get_module_config(config, module),
                                            config["General"],
                                            _session,
                                            payload,
                                        ),
                                    ),
                                )
                            )
                        except IOError:
                            logger.warn(
                                f"Portscanning result not present, skipping running module: {module.capitalize()}"
                            )

                    elif cls._target_type == INTERNAL:
                        try:
                            threads.append(
                                Thread(
                                    target=run_module,
                                    args=(
                                        cls(
                                            get_module_config(config, module),
                                            config["General"],
                                            _session,
                                            {},
                                        ),
                                    ),
                                )
                            )
                        except IOError:
                            logger.warn(
                                f"Sysaudit result not present, skipping running module: {module.capitalize()}"
                            )

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

    del threads[:]

    logger.info(f"Finished running {target_type} {type_}")


"""
Function to run all regression tests saved
"""


def regression_tests(threads, target):
    global _latest_scan

    if _latest_scan is not None:
        # Update db
        db_session = db_con()
        db_session.query(Runs). \
                filter(Runs.id == _db_run.id). \
                update({
                    "regression": RunStatus.RUNNING
                })
        db_session.commit()
        
        # Get latest scan
        latest_scan = db_session.query(Runs). \
            filter(Runs.id == _latest_scan). \
            first()
        
        # Go through all test cases
        for test in latest_scan.regression_tests:
            threads.append(
                Thread(target=run_regtest, args=(test, target))
            )

    # Update db
    db_session.query(Runs). \
            filter(Runs.id == _db_run.id). \
            update({
                "regression": RunStatus.FINISHED
            })
    db_session.commit()


"""
Main method for running the scanner for the target
"""


def scanner_start(target):
    global _session, _db_run, timer_start, _redis, _latest_scan

    # Config
    tool_conf = tool_config()["General"]

    # Get latest scan
    db_session = db_con()
    _latest_scan = db_session.query(Runs). \
        filter(Runs.target == target). \
        order_by(Runs.id.desc()). \
        first().id

    # Create db entry for run
    _db_run = Runs(target=target)
    db_session.add(_db_run)
    db_session.commit()

    print(f"----- Starting scan on {target}, SCANID: {_db_run.id}------")

    # Tracker
    if tool_conf["log_level"] == "DEBUG":
        tr = tracker.SummaryTracker()
        tr.print_diff()

    # Set logging file path
    if tool_conf["log_level"] == "DEBUG":
        level = logging.DEBUG
    elif tool_conf["log_level"] == "INFO":
        level = logging.INFO
    elif tool_conf["log_level"] == "WARN":
        level = logging.WARN
    else:
        level = logging.ERROR

    logging.basicConfig(
        filename=os.path.join(os.getcwd(), f"targets/{target}/run_{_db_run.id}.log"),
        level=level,
        format="%(asctime)s [%(levelname)s] %(message)s",
    )

    # Timer start
    logger.info("Timer started for tool execution")
    timer_start = int(round(time.time() * 1000))

    # Connect to Redis
    _redis = RedisClient(scan_id=_db_run.id)

    # Read config file for target
    config = configparser.ConfigParser()
    config.read(os.path.join(f"{os.getcwd()}/targets", target) + "/config.ini")

    # Get topology
    topology = {}
    with open(f"{os.getcwd()}/targets/{target}/topology.json") as f:
        topology = json.load(f)

    # Setup Jumpssh
    _session = Session(topology, target)
    try:
        _session.connect_ssh()
    except ConnectionError as e:
        logger.error(e)
        return False

    # Create tmp directory
    try:
        logger.info(
            f"Creating tempory directory at intermediate and target host: {tool_conf['rlocation']}"
        )
        _session.run_cmd(f"mkdir {tool_conf['rlocation']}")
        _session.run_cmd_target(f"mkdir {tool_conf['rlocation']}")
    except Exception as e:
        logger.error(e)

    # Array to hold threads
    main_threads = []

    # Start the regression tests, external and internal independant from eachother
    regression_tests(main_threads, target)
    main_threads.append(Thread(target=thread_independent_stages, args=(config, EXTERNAL,)))

    # Skip internal if SSH connection not available at target
    if _session.target_connected():
        main_threads.append(Thread(target=thread_independent_stages, args=(config, INTERNAL,)))

    for thread in main_threads:
        thread.start()

    for thread in main_threads:
        thread.join()

    del main_threads[:]

    # Remove temporary directory
    try:
        tool_conf = tool_config()["General"]
        logger.info(
            f"Removing tempory directory at intermediate and target host: {tool_conf['rlocation']}"
        )
        _session.run_cmd(f"rm -rf {tool_conf['rlocation']}")
        _session.run_cmd_target(f"rm -rf {tool_conf['rlocation']}")
    except Exception as e:
        logger.error(e)

    # Disconnect from all SSH
    _session.disconnect()

    # Set db finished at & total regression tests
    path_to_regtests = f"{os.getcwd()}/targets/{target}/regtests"
    modules = os.listdir(path_to_regtests)
    modules = [m for m in modules if m.split(".")[-1] == "json"]

    db_session.query(Runs). \
            filter(Runs.id == _db_run.id). \
            update({
                "finished": datetime.utcnow(),
                "regression_total": len(modules)
            })
    db_session.commit()

    # Disconnect from redis
    _redis.close()

    # Timer end
    logger.info(
        f"Timer ended for tool execution, took {int(round(time.time() * 1000)) - timer_start}ms to run"
    )

    # Collect garbage
    gc.collect()

    # Log memory summary
    if tool_conf["log_level"] == "DEBUG":
        tr.print_diff()

        tr.print_diff()


def thread_independent_stages(config, type):
    threads = []

    db_session = db_con()
    db_session.query(Runs). \
            filter(Runs.id == _db_run.id). \
            update({
                f"{type.lower()}_ig": RunStatus.RUNNING
            })
    db_session.commit()

    thread_module_types(threads, config, IG, type)

    db_session.query(Runs). \
            filter(Runs.id == _db_run.id). \
            update({
                f"{type.lower()}_ig": RunStatus.FINISHED,
                f"{type.lower()}_va": RunStatus.RUNNING
            })
    db_session.commit()

    thread_module_types(threads, config, VA, type)
    db_session.query(Runs). \
            filter(Runs.id == _db_run.id). \
            update({
                f"{type.lower()}_va": RunStatus.FINISHED,
                f"{type.lower()}_ex": RunStatus.RUNNING
            })
    db_session.commit()

    thread_module_types(threads, config, EX, type)
    db_session.query(Runs). \
            filter(Runs.id == _db_run.id). \
            update({
                f"{type.lower()}_ex": RunStatus.FINISHED
            })
    db_session.commit()


"""
Runs a specific module
"""


def run_module(module):
    global _redis

    module_name = remove_suffix(module.__class__.__name__, "Module")

    _redis.module_publish(module_name, ModuleStages.REMOVING, RunStatus.RUNNING)
    # Init module
    logger_class(module, "Removing old module result", "debug")
    module.remove_old_result()
    logger_class(module, "Finished removing old module", "debug")
    _redis.module_publish(module_name, ModuleStages.REMOVING, RunStatus.FINISHED)

    # Init module
    _redis.module_publish(module_name, ModuleStages.INIT, RunStatus.RUNNING)
    logger_class(module, "Initializing module", "debug")
    module.init()
    logger_class(module, "Finished initializing module", "debug")
    _redis.module_publish(module_name, ModuleStages.INIT, RunStatus.FINISHED)

    # Run module
    _redis.module_publish(module_name, ModuleStages.RUN, RunStatus.RUNNING)
    logger_class(module, "Running module", "debug")
    module.run()
    logger_class(module, "Finished running module", "debug")
    _redis.module_publish(module_name, ModuleStages.RUN, RunStatus.FINISHED)

    # Handle result
    _redis.module_publish(module_name, ModuleStages.GET, RunStatus.RUNNING)
    logger_class(module, "Getting result", "debug")
    module.get_result()
    logger_class(module, "Finished getting result", "debug")
    _redis.module_publish(module_name, ModuleStages.GET, RunStatus.FINISHED)

    # Parse module result
    _redis.module_publish(module_name, ModuleStages.PARSE, RunStatus.RUNNING)
    logger_class(module, "Parsing result", "debug")
    module.parse()
    logger_class(module, "Finished parsing result", "debug")
    _redis.module_publish(module_name, ModuleStages.PARSE, RunStatus.FINISHED)

    # Check regression
    _redis.module_publish(module_name, ModuleStages.REGTEST, RunStatus.RUNNING)
    logger_class(module, "Checking module regtests", "debug")
    module.check_regtests()
    logger_class(module, "Finished checking module regtests", "debug")
    _redis.module_publish(module_name, ModuleStages.REGTEST, RunStatus.FINISHED)


"""
Creates and runs a regtest
"""


def run_regtest(test_entry, target):
    global _session

    module_name = test_entry.module

    logger.debug(f"Running regression test {module_name.capitalize()}")

    # Get target type from module
    cls_module = getattr(
        importlib.import_module(f"modules.{module_name.lower()}"),
        f"{module_name.capitalize()}Module",
    )
    target_type = cls_module._target_type

    # Create regtest module
    cls = getattr(
        importlib.import_module(f"modules.{module_name.lower()}"),
        f"{test_entry.name}Regression",
    )

    # Create regression test object
    reg_obj = cls(_session, test_entry.payload, test_entry.config, target_type, target)

    # Remove old result
    reg_obj.remove_old_result()

    # Run regtest
    reg_obj.run()

    # Get result
    reg_obj.get_result()

    # Parse result
    reg_obj.parse()
    logger.debug(f"Finished running regression test {module_name.capitalize()}")


"""
Get CLI arguments
"""


def get_args():
    parser = argparse.ArgumentParser("python main.py")

    # parser.add_argument('-u', '--user', help="Root username", required=True)
    # parser.add_argument('-p', '--password', help="Root password", required=True)

    return parser.parse_args()


"""
Specify which target to run against
"""


def read_cli():
    target = input("Choose target: ")

    # Don't allow empty inputs
    if not target:
        return False

    try:
        # Check if valid ip adress
        socket.inet_aton(target)

        # Check if target exists
        if os.path.isdir(os.path.join(f"{os.getcwd()}/targets", target)):
            print(f"Starting scan for target: {target}")
            return target
        else:
            # Create new target
            print(f"{target} does not exists, creating new folder")

            create_new_target(target)

            print(f"Folder for {target} created, please configure it before retrying")
            return False
    except Exception:
        return False


"""
Checks that all dependencies are present
"""


def check_dependencies():
    # Check platform essentials
    if python_version() == 2:
        sys.exit("Python2 is not supported. Run the tool with Python3!")

    if "linux" not in os_name():
        sys.exit("The tool can only be run on Linux Distributions!")

    # Check if modules are installed on tool host
    # Also check if modules are installed on target machine

    # Check if DB is up & migrate new table changes
    check_db()

    # Update GTFOBIN
    if tool_config()["General"]["gtfobin_update"] == "True":
        print("Updating GTFOBIN")
        get_gtfobin()
        print("Finished updating GTFOBIN")
